{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Pocket Recommendations with Twilio, Pocket, & Spacy\n",
    "\n",
    "I currently have 92 articles in my Pocket. I often have trouble deciding what to read next, so I end up spending more time than I wish picking an article out. Instead of wasting all that time searching, I thought, _maybe I can build a tool that does that for me?_ \n",
    "\n",
    "And thus, I decided to build this idea out with Twilio. In this tutorial, we'll not only use the Twilio and Pocket APIs, but we'll also use Python's SpaCy module to match a user's requested topic.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup \n",
    "\n",
    "But before we even get started, we have to set our environment up. This guide was written in Python 3.6. If you haven't already, download [Python](https://www.python.org/downloads/) and [Pip](https://pip.pypa.io/en/stable/installing/). Next, you’ll need to install several packages that we’ll use throughout this tutorial on the command line in our project directory:\n",
    "\n",
    "``` \n",
    "pip3 install pocket==0.3.6\n",
    "pip3 install requests==2.5.0\n",
    "pip3 install spacy==1.9.0\n",
    "```\n",
    "\n",
    "We'll be using the Pocket API, which requires you first have an account. If you don’t already have one, well, first, you’ve been missing out! Secondly, sign up on their homepage [here](). This tutorial does become more exciting as you add more and more content. If you’re a new user, that’s okay! It’s up to you whether you want to add anything. \n",
    "\n",
    "Using [this]() link, we’ll make an application and generate API keys. You can name it whatever you want, but the platform we’ll be using is Web. For the purposes of this tutorial, we’ll also only need retrieval permission, but if you want to add more functionality for future reference, feel free!\n",
    "\n",
    "This should redirect you to a My Applications page. Click on the application you just created, which will redirect you to a new page that contains your consumer key right at the top, as shown below.\n",
    "\n",
    "Next, we need a request key to ultimately retrieve our access token. We can do this with [this link].\n",
    "\n",
    "```\n",
    "python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Assuming you've successfully generated your pocket API keys, we can call the pocket client to begin. Your keys are associated with your account, so these are what will provide you with the data needed for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pocket import Pocket\n",
    "\n",
    "p = Pocket(\n",
    "consumer_key='73820-b7626621174f19626b4f04fe',\n",
    "access_token='9bbd62c8-8fa2-f1ce-df16-c1d7c9'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the pocket documentation, you'll see that the `get()` method has a few parameters you can utilize. For the purposes of what we're trying to do, we'll set the parameters `since` and `state`. *Since* allows us to select a date from which to pull data from. And since we're trying to figure out how much we've read this year so far, we'll set `state` to `archive`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles():\n",
    "\n",
    "\tapi_call = p.get(contentType='article')\n",
    "\tarticles = api_call[0]['list']\n",
    "\tarticle_info = {}\n",
    "\n",
    "\tfor i in articles:\n",
    "\t\tarticle_info[articles[i]['resolved_url']] = [ articles[i]['given_title'], articles[i]['excerpt'] ]\n",
    "\treturn article_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction\n",
    "\n",
    "Information Extraction is the process of extracting the meaning from text, computationally. To accomplish this, we have to take the unstructured text and find a way to convert it into _structured_ data. With that said, information extraction is the means by which you acquire structured data from a given unstructured dataset. There are a number of ways in which this can be done, but generally, information extraction consists of searching for specific types of entities and relationships between those entities. \n",
    "\n",
    "An example is being given the following text, \n",
    "\n",
    "```\n",
    "Martin received a 98% on his math exam, whereas Jacob received a 84%. Eli, who also took the same test, received an 89%. Lastly, Ojas received a 72%.\n",
    "```\n",
    "\n",
    "This is clearly unstructured. It requires reading for any logical relationships to be understood. Through the use of information extraction techniques, however, we could output structured data such as the following: \n",
    "\n",
    "```\n",
    "Name     Grade\n",
    "Martin   98\n",
    "Jacob    84\n",
    "Eli      89\n",
    "Ojas     72\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Extraction\n",
    "\n",
    "Named entities are nouns that refer to specific types of individuals, such as organizations, people, dates, etc. Therefore, the purpose of a named entity recognition (NER) system is to identify all textual mentions of the named entities. More specifically, we'll build our own named entity recognition system with the Python module `spaCy`, a Python module commonly used for Natural Language Processing in industry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spaCy, we'll load the built-in English tokenizer, tagger, parser, NER and word vectors. We indicate this with the parameter `'en'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(article_info):\n",
    "\n",
    "\tnlp = spacy.load('en')\n",
    "\tarticle_entities = {'Default': []}\n",
    "\n",
    "\tfor i in article_info:\n",
    "\t    doc = \" \".join(article_info[i])\n",
    "\t    entities = nlp(doc)\n",
    "\t    if len(list(entities.ents)) == 0:\n",
    "\t        article_entities['Default'].append(i)\n",
    "\t        continue\n",
    "\t    for j in list(entities.ents):\n",
    "\t        try:\n",
    "\t            article_entities[str(j).lower()].append(i)\n",
    "\t        except:\n",
    "\t            article_entities[str(j).lower()] = [i]\n",
    "\treturn article_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an example to actually process, so below is some text from Columbia's website. With this example in mind, we feed it into the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(article_info):\n",
    "\n",
    "\tnlp = spacy.load('en')\n",
    "\tarticle_entities = {'Default': []}\n",
    "\n",
    "\tfor i in article_info:\n",
    "\t    doc = \" \".join(article_info[i])\n",
    "\t    entities = nlp(doc)\n",
    "\t    if len(list(entities.ents)) == 0:\n",
    "\t        article_entities['Default'].append(i)\n",
    "\t        continue\n",
    "\t    for j in list(entities.ents):\n",
    "\t        try:\n",
    "\t            article_entities[str(j).lower()].append(i)\n",
    "\t        except:\n",
    "\t            article_entities[str(j).lower()] = [i]\n",
    "\treturn article_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, redirect\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "import random \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/sms\", methods=['GET', 'POST'])\n",
    "def sms_reply():\n",
    "\tcurrent_article_set = get_articles()\n",
    "\tarticle_entities = get_entities(current_article_set)\n",
    "\n",
    "\tbody = request.values.get('Body', None)\n",
    "\n",
    "\n",
    "\tif len(body) == 0:\n",
    " \t\tarticle = article_entities['Default'][random.randint(0,len(article_entities['Default']))]\n",
    "\telse:\n",
    "\t\ttry:\n",
    "\t\t\tarticle = article_entities[body.lower()][random.randint(0, body.lower())]\n",
    "\t\texcept: \n",
    "\t\t\tarticle = article_entities['Default'][random.randint(0, len(article_entities['Default']))]\n",
    "\n",
    "\tresp = MessagingResponse()\n",
    "\n",
    "\tresp.message(article)\n",
    "\n",
    "\treturn str(resp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
